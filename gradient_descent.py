import matplotlib.pyplot as plt
import numpy as np
from random import randint
from random import random


# Setting a random seed, feel free to change it and see different solutions.
np.random.seed(42)

# TODO: Fill in code in the function below to implement a gradient descent
# step for linear regression, following a squared error rule. See the docstring
# for parameters and returned variables.
def MSEStep(X, y, W, b, learn_rate = 0.005):
    """
    This function implements the gradient descent step for squared error as a
    performance metric.
    
    Parameters
    X : array of predictor features
    y : array of outcome values
    W : predictor feature coefficients
    b : regression function intercept
    learn_rate : learning rate

    Returns
    W_new : predictor feature coefficients following gradient descent step
    b_new : intercept following gradient descent step
    """
    
    # Fill in code

    W_new = W
    b_new = b
    m = len(X)
    # for each of the points
    for i in range(m):

        # calculate error for this point
        # first calcule y1
        a = X.shape
        Xi = X[i]
        yi = y[i]
        #yh : y hat, like in the Udacity video on linear regressions
        yhi = b_new
        for j in range(len(Xi)):
            yhi += W_new[j] * Xi[j]
        
        delta_error_W =  (yi - yhi) * Xi * learn_rate
        delta_error_b =  (yi - yhi) * learn_rate

        W_new = W_new + delta_error_W
        b_new = b_new + delta_error_b
        
    print(f'W : {W}  W_new : {W_new}   b : {b}  b_new : {b_new}')

    return W_new, b_new


# The parts of the script below will be run when you press the "Test Run"
# button. The gradient descent step will be performed multiple times on
# the provided dataset, and the returned list of regression coefficients
# will be plotted.
def miniBatchGD(X, y, batch_size = 20, learn_rate = 0.005, num_iter = 25):
    """
    This function performs mini-batch gradient descent on a given dataset.

    Parameters
    X : array of predictor features
    y : array of outcome values
    batch_size : how many data points will be sampled for each iteration
    learn_rate : learning rate
    num_iter : number of batches used

    Returns
    regression_coef : array of slopes and intercepts generated by gradient
      descent procedure
    """
    n_points = X.shape[0]
    W = np.zeros(X.shape[1]) # coefficients
    b = 0 # intercept
    
    # run iterations
    regression_coef = [np.hstack((W,b))]
    for _ in range(num_iter):
        batch = np.random.choice(range(n_points), batch_size)
        X_batch = X[batch,:]
        y_batch = y[batch]
        W, b = MSEStep(X_batch, y_batch, W, b, learn_rate)
        regression_coef.append(np.hstack((W,b)))
    
    return regression_coef


def draw(data,slope=1,intercept=3):
    Xdata = data[:,0]
    Ydata = data[:,1]

    # line equation : y = slope*x + intercept
    x1 = 1
    x2 = 5
    x0 = 0
    y0 = slope * x0 + intercept
    y1 = slope * x1 + intercept
    y2 = slope * x2 + intercept
    
    # # List to hold x values.
    # x_number_values = Xdata
    # # List to hold y values.
    # y_number_values = Ydata
    # # Plot the number in the list and set the line thickness.
    # plt.plot(x_number_values, y_number_values, linewidth=3)


    # Set the line chart title and the text font size.
    plt.title(f"simple line with slope={slope} and intercept={intercept}", fontsize=19)
    # Set x axis label.
    plt.xlabel("X Values", fontsize=10)
    # Set y axis label.
    plt.ylabel("Y Values", fontsize=10)
    # Set the x, y axis tick marks text size.
    plt.tick_params(axis='both', labelsize=9)
    
    # plt.xlim(0, 10)

    plt.scatter(Xdata,Ydata, zorder = 3)
    plt.show()    


if __name__ == "__main__":
    # perform gradient descent
    print("")
    data = np.loadtxt('data.csv', delimiter = ',')

    X = data[:,:-1]
    y = data[:,-1]
    regression_coef = miniBatchGD(X, y)

    
    
    # plot the results
    import matplotlib.pyplot as plt
    
    plt.figure()
    X_min = X.min()
    X_max = X.max()
    counter = len(regression_coef)
    for W, b in regression_coef:
        counter -= 1
        color = [1 - 0.92 ** counter for _ in range(3)]
        plt.plot([X_min, X_max],[X_min * W + b, X_max * W + b], color = color)
    plt.scatter(X, y, zorder = 3)
    plt.show()

    # draw(data)

    